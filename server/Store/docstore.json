{"docstore/metadata": {"1563c2f7-bc89-4126-9642-f9d8dd14445e": {"doc_hash": "d10bb68fc3391bb843fe9a62d4cd99c9b64c1c29041d8f325a03e5c0d955b417"}, "1795ecd3-8bc9-4a56-b006-8a2013c8b622": {"doc_hash": "8b6f9c7cb125cbcb26a3a779a30d6561b4626d504ae47fec8a6503f996078104"}, "8f72ab92-b771-4834-8b41-25c8c1f29306": {"doc_hash": "40cf4816366892ba7ae7e7f7366ab40acfd991c804784953eeb005705db5fcae"}, "e0682165-e7de-4c24-8938-39db068bb33d": {"doc_hash": "a3ba707f529cd4f6d3a9d7d2b1171a73d072013197f5fc4c3503c0e440a07eca"}, "1abaf2a6-057b-41ca-aa4b-8f40589e42d8": {"doc_hash": "d4635219597b51901c00a56169d34535f2c62f26f9b4daa71fd830c8b038a215"}, "197acb94-7332-4983-803c-23a8d894874f": {"doc_hash": "61456fb0ac0e26ea388a44b7e0d7581dc8f3372bbd5f679c4e4173752c56bf78"}, "4284c00c-9316-4c5b-8d27-d435e6575c46": {"doc_hash": "6ad0b5b21844cc34c72a62b74c9672bd9d54e161d2f12e12dc3e7254a2eb7dc6"}, "c274f966-07b3-4ade-b7fa-bb54e7bc08b3": {"doc_hash": "29148c62f2d6a8b798d9c1d0f11e41b0342316539d5ccec3f064b6c288090fc6"}, "a5ff8fe0-fa88-4685-8630-24d641f8c63d": {"doc_hash": "8f011e0b32a44c9a8325c2aac9ad71c53b2a758a1fe4e73a8c389b75bffd5db3"}, "5689aca0-451c-4a91-9da0-0a3cdd5abfaa": {"doc_hash": "0b614a2c99e1f5f8eb52d7bd9ac740cda51a581a8144892c6586a55f2cfb86d3"}, "35226b90-8702-4b2c-aa60-e61ef568fe91": {"doc_hash": "ed89f2fbcb8a3f95c46fe73bc42592c0e70acd1bf7983a93b4288f2b7bf932a6"}, "23f88751-680d-4d91-bea6-3bfc3a0e5208": {"doc_hash": "31ae6851fc9e0167c294b9ecb082ca09034bd1c2c7969e3125dca5872d808cfa"}, "d2d96e7f-53d9-4c62-b912-472d80469c75": {"doc_hash": "e3938a36988c2899cf02641595020106cdc77a2358aca680ac38164ad5f8856b"}}, "docstore/data": {"8f72ab92-b771-4834-8b41-25c8c1f29306": {"__data__": {"text": "---\nsidebar: sidebar\npermalink: ehc/dro/dro-overview.html\nkeywords: NetApp Solutions, hybrid, multicloud, multi cloud, hyperscalers, vmware, disaster recovery orchestrator, DRO\nsummary:\n---\n\n= TR-4955: Disaster Recovery with FSx for ONTAP and VMC (AWS VMware Cloud)\n:hardbreaks:\n:nofooter:\n:icons: font\n:linkattrs:\n:imagesdir: ./../../media/\n\nAuthor: Niyaz Mohamed, Vedant Sethia\n\n== Overview\n\nDisaster recovery to cloud is a resilient and cost-effective way of protecting the workloads against site outages and data corruption events (for example, ransomware). With NetApp SnapMirror technology, on-premises VMware workloads can be replicated to FSx for ONTAP running in AWS.\n\nDisaster Recovery Orchestrator (DRO; a scripted solution with UI) can be used to seamlessly recover workloads replicated from on-premises to FSx for ONTAP. DRO automates the recovery from the SnapMirror level, through VM registration to VMC, to network mappings directly on NSX-T. This feature is included with all VMC environments.\n\nimage::dro-vmc-image1.png[\"This graphic depicts the structure and interconnections between an on-premises data center, a VMware Cloud on AWS SDDC instance, and Amazon FSx for NetApp ONTAP. This includes SnapMirror replication, DRaaS Ops traffic, internet or direct connect, and VMware Transit Connect.\"]\n\n== Getting started  \n\n=== Deploy and configure VMware Cloud on AWS\n\nlink:https://www.vmware.com/products/vmc-on-aws.html[VMware Cloud on AWS^] provides a cloud-native experience for VMware-based workloads in the AWS ecosystem. Each VMware Software-Defined Data Center (SDDC) runs in an Amazon Virtual Private Cloud (VPC) and provides a full VMware stack (including vCenter Server), NSX-T software-defined networking, vSAN software-defined storage, and one or more ESXi hosts that provide compute and storage resources to the workloads. To configure a VMC environment on AWS, follow the steps at this link:https://docs.netapp.com/us-en/netapp-solutions/ehc/aws/aws-setup.html[link^]. A pilot-light cluster can also be used for DR purposes. \n\nNOTE: In the initial release, DRO supports an existing pilot-light cluster. On-demand SDDC creation will be available in an upcoming release.  \n\n=== Provision and configure FSx for ONTAP\n\nAmazon FSx for NetApp ONTAP is a fully managed service that provides highly reliable, scalable, high-performing, and feature-rich file storage built on the popular NetApp ONTAP file system. Follow the steps at this link:https://docs.netapp.com/us-en/netapp-solutions/ehc/aws/aws-native-overview.html[link^] to provision and configure FSx for ONTAP.\n\n=== Deploy and configure SnapMirror to FSx for ONTAP\n\nThe next step is to use NetApp BlueXP and discover the provisioned FSx for ONTAP on AWS instance and replicate the desired datastore volumes from an on-premises environment to FSx for ONTAP with the appropriate frequency and NetApp Snapshot copy retention:\n\nimage::dro-vmc-image2.png[\"This graphic depicts the BlueXP Canvas relationship map that shows the various interactions between enabled services.\"]\n\nFollow the steps in this link to configure BlueXP. You can also use the NetApp ONTAP CLI to schedule replication following this link.\n\nNOTE: A SnapMirror relationship is a prerequisite and must be created beforehand.\n\n== DRO installation\n\nTo get started with DRO, use the Ubuntu operating system on a designated EC2 instance or virtual machine to make sure you meet the prerequisites. Then install the package.\n\n=== Prerequisites\n\n* Make sure that connectivity to the source and destination vCenter and storage systems exists.\n* DNS resolution", "doc_id": "8f72ab92-b771-4834-8b41-25c8c1f29306", "embedding": null, "doc_hash": "40cf4816366892ba7ae7e7f7366ab40acfd991c804784953eeb005705db5fcae", "extra_info": null, "node_info": {"start": 0, "end": 3599}, "relationships": {"1": "1563c2f7-bc89-4126-9642-f9d8dd14445e", "3": "e0682165-e7de-4c24-8938-39db068bb33d"}}, "__type__": "1"}, "e0682165-e7de-4c24-8938-39db068bb33d": {"__data__": {"text": "an on-premises environment to FSx for ONTAP with the appropriate frequency and NetApp Snapshot copy retention:\n\nimage::dro-vmc-image2.png[\"This graphic depicts the BlueXP Canvas relationship map that shows the various interactions between enabled services.\"]\n\nFollow the steps in this link to configure BlueXP. You can also use the NetApp ONTAP CLI to schedule replication following this link.\n\nNOTE: A SnapMirror relationship is a prerequisite and must be created beforehand.\n\n== DRO installation\n\nTo get started with DRO, use the Ubuntu operating system on a designated EC2 instance or virtual machine to make sure you meet the prerequisites. Then install the package.\n\n=== Prerequisites\n\n* Make sure that connectivity to the source and destination vCenter and storage systems exists.\n* DNS resolution should be in place if you are using DNS names. Otherwise, you should use IP addresses for the vCenter and storage systems.\n* Create a user with root permissions. You can also use sudo with an EC2 instance.\n\n=== OS requirements\n\n* Ubuntu 20.04 (LTS) with minimum of 2GB and 4 vCPUs\n* The following packages must be installed on the designated agent VM: \n** Docker \n** Docker-compose \n** Jq \n\nChange permissions on `docker.sock`: `sudo chmod 666 /var/run/docker.sock`.\n\nNOTE: The `deploy.sh` script executes all the required prerequisites.\n\n=== Install the package\n\n. Download the installation package on the designated virtual machine: \n+\n----\nhttps://github.com/NetApp-Automation/DRO.git\n----\n+\nNOTE: The agent can be installed on-premises or within an AWS VPC.\n\n. Unzip the package, run the deployment script, and enter the host IP (for example, 10.10.10.10). \n+\n----\ntar xvf DRO-prereq.tar\n----\n\n. Navigate to the directory and run the deploy script as follows:\n+\n----\nsudo sh deploy.sh  \n----\n\n\n. Access the UI using:\n+\n----\nhttps://<host-ip-address>\n----\n+\nwith the following default credentials:\n+\n----\nUsername: admin\nPassword: admin\n----\n\nNOTE: The password can be changed using the \"Change Password\" option.\n\nimage::dro-vmc-image3.png[\"Disaster Recovery Orchestrator Login screen.\"]\n\n== DRO configuration\n\nAfter FSx for ONTAP and VMC have been configured properly, you can begin configuring DRO to automate the recovery of on-premises workloads to VMC by using the read-only SnapMirror copies on FSx for ONTAP.\n\nNetApp recommends deploying the DRO agent in AWS and also to the same VPC where FSx for ONTAP is deployed (it can be peer connected too), so that the DRO agent can communicate through the network with your on-premises components as well as with the FSx for ONTAP and VMC resources.\n\nThe first step is to discover and add the on-premises and cloud resources (both vCenter and storage) to DRO. Open DRO in a supported browser and use the default username and password (admin/admin) and Add Sites. Sites can also be added using the Discover option. Add the following platforms:\n\n* On-premises\n** On-premises vCenter\n** ONTAP storage system\n* Cloud\n** VMC vCenter\n** FSx for ONTAP\n\nimage::dro-vmc-image4.png[\"Temporary placeholder image description.\"]\n\nimage::dro-vmc-image5.png[\"DRO site overview page containing Source and Destination sites.\"]\n\nOnce added, DRO performs automatic discovery and displays the VMs that have corresponding SnapMirror replicas from the source storage to FSx for ONTAP.  DRO automatically detects the networks and portgroups used by the VMs and populates them. \n\nimage::dro-vmc-image6.png[\"Automatic discovery screen containing 219 VMs and 10 datastores.\"]\n\nThe next step is to group the required VMs into functional groups to serve as resource groups.\n\n=== Resource", "doc_id": "e0682165-e7de-4c24-8938-39db068bb33d", "embedding": null, "doc_hash": "a3ba707f529cd4f6d3a9d7d2b1171a73d072013197f5fc4c3503c0e440a07eca", "extra_info": null, "node_info": {"start": 2907, "end": 6522}, "relationships": {"1": "1563c2f7-bc89-4126-9642-f9d8dd14445e", "2": "8f72ab92-b771-4834-8b41-25c8c1f29306", "3": "1abaf2a6-057b-41ca-aa4b-8f40589e42d8"}}, "__type__": "1"}, "1abaf2a6-057b-41ca-aa4b-8f40589e42d8": {"__data__": {"text": "On-premises vCenter\n** ONTAP storage system\n* Cloud\n** VMC vCenter\n** FSx for ONTAP\n\nimage::dro-vmc-image4.png[\"Temporary placeholder image description.\"]\n\nimage::dro-vmc-image5.png[\"DRO site overview page containing Source and Destination sites.\"]\n\nOnce added, DRO performs automatic discovery and displays the VMs that have corresponding SnapMirror replicas from the source storage to FSx for ONTAP.  DRO automatically detects the networks and portgroups used by the VMs and populates them. \n\nimage::dro-vmc-image6.png[\"Automatic discovery screen containing 219 VMs and 10 datastores.\"]\n\nThe next step is to group the required VMs into functional groups to serve as resource groups.\n\n=== Resource groupings\n\nAfter the platforms have been added, you can group the VMs you want to recover into resource groups. DRO resource groups allow you to group a set of dependent VMs into logical groups that contain their boot orders, boot delays, and optional application validations that can be executed upon recovery.\n\nTo start creating resource groups, complete the following steps: \n\n. Access *Resource Groups*, and click *Create New Resource Group*.\n. Under *New resource group*, select the source site from the dropdown and click *Create*.\n. Provide *Resource Group Details* and click *Continue*.\n. Select the appropriate VMs using the search option.\n. Select the boot order and boot delay (secs) for the selected VMs. Set the order of the power-on sequence by selecting each VM and setting up the priority for it. Three is the default value for all VMs.\n+\nOptions are as follows: \n+\n1 \u2013 The first virtual machine to power on\n3 \u2013 Default\n5 \u2013 The last virtual machine to power on\n\n. Click *Create Resource Group*.\n\nimage::dro-vmc-image7.png[\"Screenshot of Resource group list with two entries: Test and DemoRG1.\"]\n\n=== Replication plans\n\nYou need a plan to recover applications in the event of a disaster. Select the source and destination vCenter platforms from the drop down and pick the resource groups to be included in this plan, along with the grouping of how applications should be restored and powered on (for example, domain controllers, then tier-1, then tier-2, and so on). Such plans are sometimes also called blueprints. To define the recovery plan, navigate to the *Replication Plan* tab and click *New Replication Plan*. \n\nTo start creating a replication plan, complete the following steps:\n\n. Access *Replication Plans*, and click *Create New Replication Plan*.\n+\nimage::dro-vmc-image8.png[\"Screenshot of the replication plan screen containing one plan called DemoRP.\"]\n\n. Under *New Replication Plan*, provide a name for the plan and add recovery mappings by selecting the source site, associated vCenter, destination site, and associated vCenter.  \n+\nimage::dro-vmc-image9.png[\"Screenshot of replication plan details, including the recovery mapping.\"]\n\n. After Recovery mapping is completed, select the cluster mapping.\n+\nimage::dro-vmc-image10.png[\"Temporary placeholder image description.\"]\n\n. Select *Resource Group Details* and click *Continue*.\n\n. Set the execution order for the resource group. This option enables you to select the sequence of operations when multiple resource groups exist. \n\n. After you are done, select the network mapping to the appropriate segment. The segments should already be provisioned within VMC, so select the appropriate segment to map the VM.\t\n\n. Based on the selection of VMs, datastore mappings are automatically selected.\n+\nNOTE: SnapMirror is at the volume level. Therefore, all VMs are replicated to the replication destination. Make sure to select all VMs that are part of the datastore. If they are not selected, only the VMs that are part of the replication plan are processed.\n+\nimage::dro-vmc-image11.png[\"Temporary placeholder image description.\"]\n\n. Under the VM details, you can optionally resize the VM's CPU and RAM parameters; this can be very helpful when recovering", "doc_id": "1abaf2a6-057b-41ca-aa4b-8f40589e42d8", "embedding": null, "doc_hash": "d4635219597b51901c00a56169d34535f2c62f26f9b4daa71fd830c8b038a215", "extra_info": null, "node_info": {"start": 6602, "end": 10538}, "relationships": {"1": "1563c2f7-bc89-4126-9642-f9d8dd14445e", "2": "e0682165-e7de-4c24-8938-39db068bb33d", "3": "197acb94-7332-4983-803c-23a8d894874f"}}, "__type__": "1"}, "197acb94-7332-4983-803c-23a8d894874f": {"__data__": {"text": "sequence of operations when multiple resource groups exist. \n\n. After you are done, select the network mapping to the appropriate segment. The segments should already be provisioned within VMC, so select the appropriate segment to map the VM.\t\n\n. Based on the selection of VMs, datastore mappings are automatically selected.\n+\nNOTE: SnapMirror is at the volume level. Therefore, all VMs are replicated to the replication destination. Make sure to select all VMs that are part of the datastore. If they are not selected, only the VMs that are part of the replication plan are processed.\n+\nimage::dro-vmc-image11.png[\"Temporary placeholder image description.\"]\n\n. Under the VM details, you can optionally resize the VM's CPU and RAM parameters; this can be very helpful when recovering large environments to smaller target clusters or for conducting DR tests without having to provision a one-to-one physical VMware infrastructure. Also, you can modify the boot order and boot delay (seconds) for all the selected VMs across the resource groups. There is an additional option to modify the boot order if there are any changes required from those selected during the resource-group boot-order selection. By default, the boot order selected during resource-group selection is used; however, any modifications can be performed at this stage. \n+\nimage::dro-vmc-image12.png[\"Temporary placeholder image description.\"]\n\n. Click *Create Replication Plan*.\n+\nimage::dro-vmc-image13.png[\"Temporary placeholder image description.\"]\n\nAfter the replication plan is created, the failover option, the test-failover option, or the migrate option can be exercised depending on the requirements. During the failover and test-failover options, the most recent SnapMirror Snapshot copy is used, or a specific Snapshot copy can be selected from a point-in-time Snapshot copy (per the retention policy of SnapMirror). The point-in-time option can be very helpful if you are facing a corruption event like ransomware, where the most recent replicas are already compromised or encrypted. DRO shows all available points in time. To trigger failover or test failover with the configuration specified in the replication plan, you can click *Failover* or *Test failover*.  \n\nimage::dro-vmc-image14.png[\"Temporary placeholder image description.\"]\nimage::dro-vmc-image15.png[\"In this screen, you are provided with the Volume Snapshot details and are given the choice between using the latest snapshot and choosing a specific snapshot.\"]\n\nThe replication plan can be monitored in the task menu:\n\nimage::dro-vmc-image16.png[\"The task menu shows all jobs and options for the replication plan, and also allows you to see the logs.\"]\n\nAfter failover is triggered, the recovered items can be seen in the VMC vCenter (VMs, networks, datastores). By default, the VMs are recovered to the Workload folder.\n\nimage::dro-vmc-image17.png[\"Temporary placeholder image description.\"]\n\nFailback can be triggered at the replication-plan level. For a test failover, the tear-down option can be used to roll back the changes and remove the FlexClone relationship. Failback related to failover is a two-step process. Select the replication plan and select *Reverse data sync*. \n\nimage::dro-vmc-image18.png[\"Screenshot of Replication Plan overview with dropdown containing Reverse Data Sync option.\"]\nimage::dro-vmc-image19.png[\"Temporary placeholder image description.\"]\n\nOnce completed, you can trigger failback to move back to original production site.\n\nimage::dro-vmc-image20.png[\"Screenshot of Replication Plan overview with dropdown containing the Failback option.\"]\nimage::dro-vmc-image21.png[\"Screenshot of DRO summary page with original production site up and running.\"]\n\nFrom NetApp BlueXP, we can see that replication health has broken off for the appropriate volumes (those that were mapped to VMC as read-write volumes).  During test failover, DRO does not map the destination or replica volume. Instead, it makes a FlexClone copy of the required SnapMirror (or", "doc_id": "197acb94-7332-4983-803c-23a8d894874f", "embedding": null, "doc_hash": "61456fb0ac0e26ea388a44b7e0d7581dc8f3372bbd5f679c4e4173752c56bf78", "extra_info": null, "node_info": {"start": 10486, "end": 14508}, "relationships": {"1": "1563c2f7-bc89-4126-9642-f9d8dd14445e", "2": "1abaf2a6-057b-41ca-aa4b-8f40589e42d8", "3": "4284c00c-9316-4c5b-8d27-d435e6575c46"}}, "__type__": "1"}, "4284c00c-9316-4c5b-8d27-d435e6575c46": {"__data__": {"text": "of Replication Plan overview with dropdown containing Reverse Data Sync option.\"]\nimage::dro-vmc-image19.png[\"Temporary placeholder image description.\"]\n\nOnce completed, you can trigger failback to move back to original production site.\n\nimage::dro-vmc-image20.png[\"Screenshot of Replication Plan overview with dropdown containing the Failback option.\"]\nimage::dro-vmc-image21.png[\"Screenshot of DRO summary page with original production site up and running.\"]\n\nFrom NetApp BlueXP, we can see that replication health has broken off for the appropriate volumes (those that were mapped to VMC as read-write volumes).  During test failover, DRO does not map the destination or replica volume. Instead, it makes a FlexClone copy of the required SnapMirror (or Snapshot) instance and exposes the FlexClone instance, which does not consume additional physical capacity for FSx for ONTAP. This process makes sure that the volume is not modified and replica jobs can continue even during DR tests or triage workflows. Additionally, this process makes sure that, if errors occur or corrupted data is recovered, the recovery can be cleaned up without the risk of the replica being destroyed.\n\nimage::dro-vmc-image22.png[\"Temporary placeholder image description.\"]\n\n=== Ransomware recovery\n\nRecovering from ransomware can be a daunting task. Specifically, it can be hard for IT organizations to pin-point where the safe point of return is and, once that is determined, to protect recovered workloads from reoccurring attacks from, for example, sleeping malware or vulnerable applications.\n\nDRO addresses these concerns by enabling you to recover your system from any available point in time. You can also recover workloads to functional and yet isolated networks so that applications can function and communicate with each other in a location where they are not exposed to north-south traffic. This gives your security team a safe place to conduct forensics and make sure there is no hidden or sleeping malware.\n\n== Benefits\n* Use of the efficient and resilient SnapMirror replication.\n* Recovery to any available point in time with Snapshot copy retention.\n* Full automation of all required steps to recover hundreds to thousands of VMs from the storage, compute, network, and application validation steps.\n* Workload recovery with ONTAP FlexClone technology using a method that doesn't change the replicated volume.\n** Avoids risk of data corruption for volumes or Snapshot copies.\n** Avoids replication interruptions during DR test workflows.\n** Potential use of DR data with cloud computing resources for workflows beyond DR such as DevTest, security testing, patch or upgrade testing, and remediation testing.\n* CPU and RAM optimization to help lower cloud costs by allowing recovery to smaller compute clusters.", "doc_id": "4284c00c-9316-4c5b-8d27-d435e6575c46", "embedding": null, "doc_hash": "6ad0b5b21844cc34c72a62b74c9672bd9d54e161d2f12e12dc3e7254a2eb7dc6", "extra_info": null, "node_info": {"start": 14420, "end": 17226}, "relationships": {"1": "1563c2f7-bc89-4126-9642-f9d8dd14445e", "2": "197acb94-7332-4983-803c-23a8d894874f"}}, "__type__": "1"}, "c274f966-07b3-4ade-b7fa-bb54e7bc08b3": {"__data__": {"text": "\n=====\nCVO Single Node Deployment\n--\n\n\n.Terraform configuration files for deployment of NetApp CVO (Single Node Instance) on AWS\n\nThis section contains various Terraform configuration files to deploy/configure single node NetApp CVO (Cloud Volumes ONTAP) on AWS (Amazon Web Services).\n\nTerraform Documentation: https://registry.terraform.io/providers/NetApp/netapp-cloudmanager/latest/docs\n\n\n\n\n.Procedure\nIn order to run the template:\n\n.. Clone the repository.\n+\n[source, cli]\n    git clone https://github.com/NetApp-Automation/na_cloud_volumes_automation.git\n\n.. Navigate to the desired folder\n+\n[source, cli]\n    cd na_cloud_volumes_automation/\n\n\n.. Configure AWS credentials from the CLI.\n+\n[source, cli]\n    aws configure\n\n    - AWS Access Key ID [None]: accesskey\n    - AWS Secret Access Key [None]: secretkey\n    - Default region name [None]: us-west-2\n    - Default output format [None]: json\n\n\n.. Update the variable values in `vars/aws_cvo_single_node_deployment.tfvar`\n+\nNOTE: You can choose to deploy the connector by setting the variable \"aws_connector_deploy_bool\" value to true/false.\n\n.. Initialize the Terraform repository to install all the pre-requisites and prepare for deployment.\n+\n[source, cli]\n    terraform init\n\n\n.. Verify the terraform files using terraform validate command.\n+\n[source, cli]\n    terraform validate\n\n.. Make a dry run of the configuration to get a preview of all the changes expected by the deployment.\n+\n[source, cli]\n    terraform plan -target=\"module.aws_sn\" -var-file=\"vars/aws_cvo_single_node_deployment.tfvars\"\n\n\n.. Run the deployment\n+\n[source, cli]\n    terraform apply -target=\"module.aws_sn\" -var-file=\"vars/aws_cvo_single_node_deployment.tfvars\"\n\n\nTo delete the deployment\n\n[source, cli]\n    terraform destroy\n\n\n.Recipies:\n\n\n`Connector`\n\nTerraform variables for NetApp AWS connector instance for CVO deployment.\n\n|===\n| *Name* | *Type* | *Description*\n| *aws_connector_deploy_bool* | Bool | (Required) Check for Connector deployment.\n| *aws_connector_name* | String | (Required) The name of the Cloud Manager Connector.\n| *aws_connector_region* | String | (Required) The region where the Cloud Manager Connector will be created.\n| *aws_connector_key_name* | String | (Required) The name of the key pair to use for the Connector instance.\n| *aws_connector_company* | String | (Required) The name of the company of the user.\n| *aws_connector_instance_type* | String | (Required) The type of instance (for example, t3.xlarge). At least 4 CPU and 16 GB of memory are required.\n| *aws_connector_subnet_id* | String | (Required) The ID of the subnet for the instance.\n| *aws_connector_security_group_id* | String | (Required) The ID of the security group for the instance, multiple security groups can be provided separated by ','.\n| *aws_connector_iam_instance_profile_name* | String | (Required) The name of the instance profile for the Connector.\n| *aws_connector_account_id* | String | (Optional) The NetApp account ID that the Connector will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n|", "doc_id": "c274f966-07b3-4ade-b7fa-bb54e7bc08b3", "embedding": null, "doc_hash": "29148c62f2d6a8b798d9c1d0f11e41b0342316539d5ccec3f064b6c288090fc6", "extra_info": null, "node_info": {"start": 0, "end": 3228}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "3": "a5ff8fe0-fa88-4685-8630-24d641f8c63d"}}, "__type__": "1"}, "a5ff8fe0-fa88-4685-8630-24d641f8c63d": {"__data__": {"text": "At least 4 CPU and 16 GB of memory are required.\n| *aws_connector_subnet_id* | String | (Required) The ID of the subnet for the instance.\n| *aws_connector_security_group_id* | String | (Required) The ID of the security group for the instance, multiple security groups can be provided separated by ','.\n| *aws_connector_iam_instance_profile_name* | String | (Required) The name of the instance profile for the Connector.\n| *aws_connector_account_id* | String | (Optional) The NetApp account ID that the Connector will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n| *aws_connector_public_ip_bool* | Bool | (Optional) \u202fIndicates whether to associate a public IP address to the instance. If not provided, the association will be done based on the subnet's configuration.\n|===\n\n`Single Node Instance`\n\nTerraform variables for single NetApp CVO instance.\n\n|===\n| *Name* | *Type* | *Description*\n| *cvo_name* | String | (Required) The name of the Cloud Volumes ONTAP working environment.\n| *cvo_region* | String | (Required) The region where the working environment will be created.\n| *cvo_subnet_id* | String | (Required) The subnet id where the working environment will be created.\n| *cvo_vpc_id* | String | (Optional) The VPC ID where the working environment will be created. If this argument isn't provided, the VPC will be calculated by using the provided subnet ID.\n| *cvo_svm_password* | String | (Required) The admin password for Cloud Volumes ONTAP.\n| *cvo_writing_speed_state* | String | (Optional) The write speed setting for Cloud Volumes ONTAP: ['NORMAL','HIGH']. The default is 'NORMAL'.\n|===\n\n--\n.CVO HA Deployment\n--\n.Terraform configuration files for deployment of NetApp CVO (HA Pair) on AWS\n\nThis section contains various Terraform configuration files to deploy/configure NetApp CVO (Cloud Volumes ONTAP) in high availability pair on AWS (Amazon Web Services).\n\nTerraform Documentation: https://registry.terraform.io/providers/NetApp/netapp-cloudmanager/latest/docs\n\n.Procedure\nIn order to run the template:\n\n.. Clone the repository.\n+\n[source, cli]\n    git clone https://github.com/NetApp-Automation/na_cloud_volumes_automation.git\n\n.. Navigate to the desired folder\n+\n[source, cli]\n    cd na_cloud_volumes_automation/\n\n.. Configure AWS credentials from the CLI.\n+\n[source, cli]\n    aws configure\n\n    - AWS Access Key ID [None]: accesskey\n    - AWS Secret Access Key [None]: secretkey\n    - Default region name [None]: us-west-2\n    - Default output format [None]: json\n\n.. Update the variable values in `vars/aws_cvo_ha_deployment.tfvars`.\n+\nNOTE: You can choose to deploy the connector by setting the variable \"aws_connector_deploy_bool\" value to true/false.\n\n.. Initialize the Terraform repository to install all the pre-requisites and prepare for deployment.\n+\n[source, cli]\n      terraform init\n\n.. Verify the terraform files using terraform validate command.\n+\n[source, cli]\n    terraform validate\n\n.. Make a dry run of the configuration to get a preview of all the changes expected by the deployment.\n+\n[source, cli]\n    terraform plan -target=\"module.aws_ha\" -var-file=\"vars/aws_cvo_ha_deployment.tfvars\"\n\n.. Run the", "doc_id": "a5ff8fe0-fa88-4685-8630-24d641f8c63d", "embedding": null, "doc_hash": "8f011e0b32a44c9a8325c2aac9ad71c53b2a758a1fe4e73a8c389b75bffd5db3", "extra_info": null, "node_info": {"start": 2586, "end": 5913}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "2": "c274f966-07b3-4ade-b7fa-bb54e7bc08b3", "3": "5689aca0-451c-4a91-9da0-0a3cdd5abfaa"}}, "__type__": "1"}, "5689aca0-451c-4a91-9da0-0a3cdd5abfaa": {"__data__": {"text": "You can choose to deploy the connector by setting the variable \"aws_connector_deploy_bool\" value to true/false.\n\n.. Initialize the Terraform repository to install all the pre-requisites and prepare for deployment.\n+\n[source, cli]\n      terraform init\n\n.. Verify the terraform files using terraform validate command.\n+\n[source, cli]\n    terraform validate\n\n.. Make a dry run of the configuration to get a preview of all the changes expected by the deployment.\n+\n[source, cli]\n    terraform plan -target=\"module.aws_ha\" -var-file=\"vars/aws_cvo_ha_deployment.tfvars\"\n\n.. Run the deployment\n+\n[source, cli]\n    terraform apply -target=\"module.aws_ha\" -var-file=\"vars/aws_cvo_ha_deployment.tfvars\"\n\n\nTo delete the deployment\n\n[source, cli]\n    terraform destroy\n\n\n.Recipies:\n\n`Connector`\n\nTerraform variables for NetApp AWS connector instance for CVO deployment.\n\n|===\n| *Name* | *Type* | *Description*\n| *aws_connector_deploy_bool* | Bool | (Required) Check for Connector deployment.\n| *aws_connector_name* | String | (Required) The name of the Cloud Manager Connector.\n| *aws_connector_region* | String | (Required) The region where the Cloud Manager Connector will be created.\n| *aws_connector_key_name* | String | (Required) The name of the key pair to use for the Connector instance.\n| *aws_connector_company* | String | (Required) The name of the company of the user.\n| *aws_connector_instance_type* | String | (Required) The type of instance (for example, t3.xlarge). At least 4 CPU and 16 GB of memory are required.\n| *aws_connector_subnet_id* | String | (Required) The ID of the subnet for the instance.\n| *aws_connector_security_group_id* | String | (Required) The ID of the security group for the instance, multiple security groups can be provided separated by ','.\n| *aws_connector_iam_instance_profile_name* | String | (Required) The name of the instance profile for the Connector.\n| *aws_connector_account_id* | String | (Optional) The NetApp account ID that the Connector will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n| *aws_connector_public_ip_bool* | Bool | (Optional) \u202fIndicates whether to associate a public IP address to the instance. If not provided, the association will be done based on the subnet's configuration.\n|===\n\n\n`HA Pair`\n\nTerraform variables for NetApp CVO instances in HA Pair.\n\n|===\n| *Name* | *Type* | *Description*\n| *cvo_is_ha* | Bool | (Optional) Indicate whether the working environment is an HA pair or not [true, false]. The default is false.\n| *cvo_name* | String | (Required) The name of the Cloud Volumes ONTAP working environment.\n| *cvo_region* | String | (Required) The region where the working environment will be created.\n| *cvo_node1_subnet_id* | String | (Required) The subnet id where the first node will be created.\n| *cvo_node2_subnet_id* | String | (Required) The subnet id where the second node will be created.\n| *cvo_vpc_id* | String | (Optional) The VPC ID where the working environment will be created. If this argument isn't provided, the VPC will be calculated by using the provided subnet ID.\n| *cvo_svm_password* | String | (Required) The admin password for Cloud Volumes ONTAP.\n| *cvo_failover_mode*", "doc_id": "5689aca0-451c-4a91-9da0-0a3cdd5abfaa", "embedding": null, "doc_hash": "0b614a2c99e1f5f8eb52d7bd9ac740cda51a581a8144892c6586a55f2cfb86d3", "extra_info": null, "node_info": {"start": 6060, "end": 9414}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "2": "a5ff8fe0-fa88-4685-8630-24d641f8c63d", "3": "35226b90-8702-4b2c-aa60-e61ef568fe91"}}, "__type__": "1"}, "35226b90-8702-4b2c-aa60-e61ef568fe91": {"__data__": {"text": "| String | (Required) The name of the Cloud Volumes ONTAP working environment.\n| *cvo_region* | String | (Required) The region where the working environment will be created.\n| *cvo_node1_subnet_id* | String | (Required) The subnet id where the first node will be created.\n| *cvo_node2_subnet_id* | String | (Required) The subnet id where the second node will be created.\n| *cvo_vpc_id* | String | (Optional) The VPC ID where the working environment will be created. If this argument isn't provided, the VPC will be calculated by using the provided subnet ID.\n| *cvo_svm_password* | String | (Required) The admin password for Cloud Volumes ONTAP.\n| *cvo_failover_mode* | String | (Optional) For HA, the failover mode for the HA pair: ['PrivateIP', 'FloatingIP']. 'PrivateIP' is for a single availability zone and 'FloatingIP' is for multiple availability zones.\n| *cvo_mediator_subnet_id* | String | (Optional) For HA, the subnet ID of the mediator.\n| *cvo_mediator_key_pair_name* | String | (Optional) For HA, the key pair name for the mediator instance.\n| *cvo_cluster_floating_ip* | String | (Optional) For HA FloatingIP, the cluster management floating IP address.\n| *cvo_data_floating_ip* | String | (Optional) For HA FloatingIP, the data floating IP address.\n| *cvo_data_floating_ip2* | String | (Optional) For HA FloatingIP, the data floating IP address.\n| *cvo_svm_floating_ip* | String | (Optional) For HA FloatingIP, the SVM management floating IP address.\n| *cvo_route_table_ids* | List | (Optional) For HA FloatingIP, the list of route table IDs that will be updated with the floating IPs.\n|===\n\n--\n.FSx Deployment\n--\n.Terraform configuration files for deployment of NetApp ONTAP FSx on AWS\nThis section contains various Terraform configuration files to deploy/configure NetApp ONTAP FSx on AWS (Amazon Web Services).\n\nTerraform Documentation: https://registry.terraform.io/providers/NetApp/netapp-cloudmanager/latest/docs\n\n.Procedure\nIn order to run the template:\n\n.. Clone the repository.\n+\n[source, cli]\n    git clone https://github.com/NetApp-Automation/na_cloud_volumes_automation.git\n\n.. Navigate to the desired folder\n+\n[source, cli]\n    cd na_cloud_volumes_automation/\n\n.. Configure AWS credentials from the CLI.\n+\n[source, cli]\n    aws configure\n\n    - AWS Access Key ID [None]: accesskey\n    - AWS Secret Access Key [None]: secretkey\n    - Default region name [None]: us-west-2\n    - Default output format [None]:\n\n.. Update the variable values in `vars/aws_fsx_deployment.tfvars`\n+\nNOTE: You can choose to deploy the connector by setting the variable \"aws_connector_deploy_bool\" value to true/false.\n\n.. Initialize the Terraform repository to install all the pre-requisites and prepare for deployment.\n+\n[source, cli]\n    terraform init\n\n.. Verify the terraform files using terraform validate command.\n+\n[source, cli]\n    terraform validate\n\n.. Make a dry run of the configuration to get a preview of all the changes expected by the deployment.\n+\n[source, cli]\n    terraform plan -target=\"module.aws_fsx\" -var-file=\"vars/aws_fsx_deployment.tfvars\"\n\n.. Run the deployment\n+\n[source, cli]\n    terraform apply -target=\"module.aws_fsx\"", "doc_id": "35226b90-8702-4b2c-aa60-e61ef568fe91", "embedding": null, "doc_hash": "ed89f2fbcb8a3f95c46fe73bc42592c0e70acd1bf7983a93b4288f2b7bf932a6", "extra_info": null, "node_info": {"start": 9345, "end": 12498}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "2": "5689aca0-451c-4a91-9da0-0a3cdd5abfaa", "3": "23f88751-680d-4d91-bea6-3bfc3a0e5208"}}, "__type__": "1"}, "23f88751-680d-4d91-bea6-3bfc3a0e5208": {"__data__": {"text": "value to true/false.\n\n.. Initialize the Terraform repository to install all the pre-requisites and prepare for deployment.\n+\n[source, cli]\n    terraform init\n\n.. Verify the terraform files using terraform validate command.\n+\n[source, cli]\n    terraform validate\n\n.. Make a dry run of the configuration to get a preview of all the changes expected by the deployment.\n+\n[source, cli]\n    terraform plan -target=\"module.aws_fsx\" -var-file=\"vars/aws_fsx_deployment.tfvars\"\n\n.. Run the deployment\n+\n[source, cli]\n    terraform apply -target=\"module.aws_fsx\" -var-file=\"vars/aws_fsx_deployment.tfvars\"\n\nTo delete the deployment\n\n[source, cli]\n    terraform destroy\n\n\n.Recipes:\n\n`Connector`\n\nTerraform variables for NetApp AWS connector instance.\n\n|===\n| *Name* | *Type* | *Description*\n| *aws_connector_deploy_bool* | Bool | (Required) Check for Connector deployment.\n| *aws_connector_name* | String | (Required) The name of the Cloud Manager Connector.\n| *aws_connector_region* | String | (Required) The region where the Cloud Manager Connector will be created.\n| *aws_connector_key_name* | String | (Required) The name of the key pair to use for the Connector instance.\n| *aws_connector_company* | String | (Required) The name of the company of the user.\n| *aws_connector_instance_type* | String | (Required) The type of instance (for example, t3.xlarge). At least 4 CPU and 16 GB of memory are required.\n| *aws_connector_subnet_id* | String | (Required) The ID of the subnet for the instance.\n| *aws_connector_security_group_id* | String | (Required) The ID of the security group for the instance, multiple security groups can be provided separated by ','.\n| *aws_connector_iam_instance_profile_name* | String | (Required) The name of the instance profile for the Connector.\n| *aws_connector_account_id* | String | (Optional) The NetApp account ID that the Connector will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n| *aws_connector_public_ip_bool* | Bool | (Optional) \u202fIndicates whether to associate a public IP address to the instance. If not provided, the association will be done based on the subnet's configuration.\n|===\n\n`FSx Instance`\n\nTerraform variables for NetApp ONTAP FSx instance.\n\n|===\n| *Name* | *Type* | *Description*\n| *fsx_name* | String | (Required) The name of the Cloud Volumes ONTAP working environment.\n| *fsx_region* | String | (Required) The region where the working environment will be created.\n| *fsx_primary_subnet_id* | String | (Required) The primary subnet id where the working environment will be created.\n| *fsx_secondary_subnet_id* | String | (Required) The secondary subnet id where the working environment will be created.\n| *fsx_account_id* | String | (Required) The NetApp account ID that the FSx instance will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n| *fsx_workspace_id* | String | (Required) The ID of the Cloud Manager workspace of working environment.\n| *fsx_admin_password* | String | (Required) The admin password for Cloud Volumes ONTAP.\n| *fsx_throughput_capacity* | String | (Optional) capacity of the throughput.\n|", "doc_id": "23f88751-680d-4d91-bea6-3bfc3a0e5208", "embedding": null, "doc_hash": "31ae6851fc9e0167c294b9ecb082ca09034bd1c2c7969e3125dca5872d808cfa", "extra_info": null, "node_info": {"start": 12586, "end": 16029}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "2": "35226b90-8702-4b2c-aa60-e61ef568fe91", "3": "d2d96e7f-53d9-4c62-b912-472d80469c75"}}, "__type__": "1"}, "d2d96e7f-53d9-4c62-b912-472d80469c75": {"__data__": {"text": "working environment will be created.\n| *fsx_secondary_subnet_id* | String | (Required) The secondary subnet id where the working environment will be created.\n| *fsx_account_id* | String | (Required) The NetApp account ID that the FSx instance will be associated with. If not provided, Cloud Manager uses the first account. If no account exists, Cloud Manager creates a new account. You can find the account ID in the account tab of Cloud Manager at\u202fhttps://cloudmanager.netapp.com.\n| *fsx_workspace_id* | String | (Required) The ID of the Cloud Manager workspace of working environment.\n| *fsx_admin_password* | String | (Required) The admin password for Cloud Volumes ONTAP.\n| *fsx_throughput_capacity* | String | (Optional) capacity of the throughput.\n| *fsx_storage_capacity_size* | String | (Optional) EBS volume size for the first data aggregate. For GB, the unit can be: [100 or 500]. For TB, the unit can be: [1,2,4,8,16]. The default is '1'\n| *fsx_storage_capacity_size_unit* | String | (Optional) ['GB' or 'TB']. The default is 'TB'.\n| *fsx_cloudmanager_aws_credential_name* | String | (Required) The name of the AWS Credentials account name.\n|===\n\n\n\n--\n=====", "doc_id": "d2d96e7f-53d9-4c62-b912-472d80469c75", "embedding": null, "doc_hash": "e3938a36988c2899cf02641595020106cdc77a2358aca680ac38164ad5f8856b", "extra_info": null, "node_info": {"start": 15754, "end": 16922}, "relationships": {"1": "1795ecd3-8bc9-4a56-b006-8a2013c8b622", "2": "23f88751-680d-4d91-bea6-3bfc3a0e5208"}}, "__type__": "1"}}}